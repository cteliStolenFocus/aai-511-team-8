{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d421b3bb",
   "metadata": {},
   "source": [
    "# Task Breakdown\n",
    "\n",
    "- Data Collection: Data is collected and provided to you.\n",
    "- Data Pre-processing: Convert the musical scores into a format suitable for deep learning models. This involves converting the musical scores into MIDI files and applying data augmentation techniques.\n",
    "- Feature Extraction: Extractfeatures from the MIDI files, such as notes, chords, and tempo, using music analysis tools.\n",
    "- Model Building: Develop a deep learning model using LSTM and CNN architectures to classify the musical scores according to the composer.\n",
    "- Model Training: Train the deep learning model using the pre-processed and feature-extracted data.\n",
    "- Model Evaluation: Evaluate the performance of the deep learning model using accuracy, precision, and recall metrics.\n",
    "- Model Optimization: Optimize the deep learning model by fine-tuning hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4994e0e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pretty_midi in c:\\users\\chris\\appdata\\roaming\\python\\python39\\site-packages (0.2.10)\n",
      "Requirement already satisfied: numpy>=1.7.0 in c:\\tools\\anaconda3\\lib\\site-packages (from pretty_midi) (1.21.5)\n",
      "Requirement already satisfied: six in c:\\tools\\anaconda3\\lib\\site-packages (from pretty_midi) (1.16.0)\n",
      "Requirement already satisfied: mido>=1.1.16 in c:\\users\\chris\\appdata\\roaming\\python\\python39\\site-packages (from pretty_midi) (1.2.10)\n"
     ]
    }
   ],
   "source": [
    "!pip install pretty_midi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "693ec580",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title 1: Functions for Data Collection\n",
    "# Function for Feature extractions\n",
    "import os\n",
    "import glob\n",
    "import pretty_midi\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Check if pickle file exists and use the file for dataset\n",
    "import pickle\n",
    "\n",
    "\n",
    "# Directory of MIDI files\n",
    "base_dir = 'train'\n",
    "\n",
    "# Specify the path and filename of the pickle file\n",
    "# Since the pkl file exists, that means the data was processed and features extracted\n",
    "pickle_file_name = 'team8_composer_dataset.pkl'\n",
    "\n",
    "# TBD:Extract features using librosa for further feature extraction\n",
    "def calculate_features(midi_file):\n",
    "    # Load MIDI file\n",
    "    midi_data = pretty_midi.PrettyMIDI(midi_file)\n",
    "\n",
    "    # Time interval for calculating features\n",
    "    interval = 1.0  # 1 second\n",
    "    times = np.arange(0, midi_data.get_end_time(), interval)\n",
    "\n",
    "    # Create arrays for storing time series data\n",
    "    pitch = np.zeros(len(times))\n",
    "    volume = np.zeros(len(times))\n",
    "    note_density = np.zeros(len(times))\n",
    "    tempo = np.zeros(len(times))\n",
    "\n",
    "    # Calculate time series data for each feature\n",
    "    for i, t in enumerate(times):\n",
    "        # Get notes that are playing at this time\n",
    "        notes = [note for note in midi_data.instruments[0].notes if note.start <= t < note.end]\n",
    "\n",
    "        # Calculate average pitch\n",
    "        if notes:\n",
    "            pitch[i] = np.mean([note.pitch for note in notes])\n",
    "\n",
    "        # Calculate note density (notes per second)\n",
    "        note_density[i] = len(notes) / interval\n",
    "\n",
    "        # Calculate average volume\n",
    "        if notes:\n",
    "            volume[i] = np.mean([note.velocity for note in notes])\n",
    "\n",
    "    # Calculate rhythmic complexity (variance in inter-onset intervals)\n",
    "    inter_onset_intervals = np.diff([note.start for note in midi_data.instruments[0].notes])\n",
    "    rhythmic_complexity = np.var(inter_onset_intervals)\n",
    "\n",
    "    # Calculate tempo for each moment in time\n",
    "    tempo_changes = midi_data.get_tempo_changes()\n",
    "    tempo = np.interp(times, tempo_changes[0], tempo_changes[1])\n",
    "\n",
    "    return times, pitch, note_density, volume, rhythmic_complexity, tempo\n",
    "\n",
    "def process_composer_data():\n",
    "    # Initialize DataFrame\n",
    "    df = pd.DataFrame(columns=[\"Composer\",\"Times\", \"Pitch\", \"Note_Density\", \"Volume\", \n",
    "                               \"Rhythmic_Complexity\", \"Tempo\"])\n",
    "\n",
    "    # Iterate over all composer directories\n",
    "    for composer_dir in glob.glob(os.path.join(base_dir, '*')):\n",
    "        # Get the composer's name\n",
    "        composer_name = os.path.basename(composer_dir)\n",
    "        print(f\"Processing {composer_name} MIDI files...\")\n",
    "\n",
    "        # Iterate over all MIDI files in composer's directory\n",
    "        for midi_file in glob.glob(os.path.join(composer_dir, '*.mid')):\n",
    "            print(f\"Processing {midi_file}...\")\n",
    "            try:\n",
    "                times, pitch, note_density, volume, rhythmic_complexity, tempo = calculate_features(midi_file)\n",
    "                # Append to DataFrame\n",
    "                df = df.append({\"Composer\": composer_name, \"Times\": times, \"Pitch\": pitch, \n",
    "                                \"Note_Density\": note_density, \"Volume\": volume, \n",
    "                                \"Rhythmic_Complexity\": rhythmic_complexity, \n",
    "                                \"Tempo\": tempo}, \n",
    "                               ignore_index=True)\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {midi_file}: {str(e)}\")\n",
    "    \n",
    "    # Write the DataFrame to a pickle file\n",
    "    df.to_pickle(base_dir + \"/\" + pickle_file_name)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b337b1c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Composer                                              Times  \\\n",
      "0     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "1     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "2     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "3     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "4     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "\n",
      "                                               Pitch  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 73.0,...   \n",
      "1  [0.0, 78.0, 78.0, 0.0, 66.0, 66.0, 0.0, 0.0, 6...   \n",
      "2  [0.0, 67.0, 43.0, 0.0, 67.0, 0.0, 43.0, 55.0, ...   \n",
      "3  [0.0, 0.0, 50.0, 74.0, 0.0, 74.0, 0.0, 74.0, 0...   \n",
      "4  [0.0, 63.0, 69.0, 0.0, 0.0, 69.0, 0.0, 45.0, 0...   \n",
      "\n",
      "                                        Note_Density  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
      "1  [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, ...   \n",
      "2  [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...   \n",
      "3  [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...   \n",
      "4  [0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...   \n",
      "\n",
      "                                              Volume  Rhythmic_Complexity  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 127.0...             4.609656   \n",
      "1  [0.0, 127.0, 127.0, 0.0, 127.0, 127.0, 0.0, 0....             1.625026   \n",
      "2  [0.0, 127.0, 127.0, 0.0, 127.0, 0.0, 127.0, 12...             2.137464   \n",
      "3  [0.0, 0.0, 127.0, 127.0, 0.0, 127.0, 0.0, 127....             1.291966   \n",
      "4  [0.0, 127.0, 127.0, 0.0, 0.0, 127.0, 0.0, 127....             0.866606   \n",
      "\n",
      "                                               Tempo  \n",
      "0  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "1  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "2  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "3  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "4  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n"
     ]
    }
   ],
   "source": [
    "# Data Processing\n",
    "# Feature extraction\n",
    "\n",
    "pickle_file = base_dir + \"/\" + pickle_file_name\n",
    "# Check if the pickle file exists\n",
    "if not os.path.exists(pickle_file):\n",
    "    print(\"Music Data not Pickled, creating dataset using feature extract.\")\n",
    "    df = process_composer_data()\n",
    "else:\n",
    "    # Open the pickle file in binary mode and load the data\n",
    "    with open(pickle_file, 'rb') as file:\n",
    "        data = pickle.load(file)\n",
    "\n",
    "    # Create a DataFrame from the loaded data\n",
    "    df = pd.DataFrame(data)\n",
    "\n",
    "    # Now you have your DataFrame ready for use\n",
    "    print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f685c16e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Composer                                              Times  \\\n",
      "0     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "1     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "2     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "3     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "4     bach  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...   \n",
      "\n",
      "                                               Pitch  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 73.0,...   \n",
      "1  [0.0, 78.0, 78.0, 0.0, 66.0, 66.0, 0.0, 0.0, 6...   \n",
      "2  [0.0, 67.0, 43.0, 0.0, 67.0, 0.0, 43.0, 55.0, ...   \n",
      "3  [0.0, 0.0, 50.0, 74.0, 0.0, 74.0, 0.0, 74.0, 0...   \n",
      "4  [0.0, 63.0, 69.0, 0.0, 0.0, 69.0, 0.0, 45.0, 0...   \n",
      "\n",
      "                                        Note_Density  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 1.0, ...   \n",
      "1  [0.0, 1.0, 1.0, 0.0, 1.0, 1.0, 0.0, 0.0, 1.0, ...   \n",
      "2  [0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 0.0, ...   \n",
      "3  [0.0, 0.0, 1.0, 1.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...   \n",
      "4  [0.0, 2.0, 1.0, 0.0, 0.0, 1.0, 0.0, 1.0, 0.0, ...   \n",
      "\n",
      "                                              Volume  Rhythmic_Complexity  \\\n",
      "0  [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 127.0...             4.609656   \n",
      "1  [0.0, 127.0, 127.0, 0.0, 127.0, 127.0, 0.0, 0....             1.625026   \n",
      "2  [0.0, 127.0, 127.0, 0.0, 127.0, 0.0, 127.0, 12...             2.137464   \n",
      "3  [0.0, 0.0, 127.0, 127.0, 0.0, 127.0, 0.0, 127....             1.291966   \n",
      "4  [0.0, 127.0, 127.0, 0.0, 0.0, 127.0, 0.0, 127....             0.866606   \n",
      "\n",
      "                                               Tempo  \n",
      "0  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "1  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "2  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "3  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "4  [120.0, 120.0, 120.0, 120.0, 120.0, 120.0, 120...  \n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 369 entries, 0 to 368\n",
      "Data columns (total 7 columns):\n",
      " #   Column               Non-Null Count  Dtype  \n",
      "---  ------               --------------  -----  \n",
      " 0   Composer             369 non-null    object \n",
      " 1   Times                369 non-null    object \n",
      " 2   Pitch                369 non-null    object \n",
      " 3   Note_Density         369 non-null    object \n",
      " 4   Volume               369 non-null    object \n",
      " 5   Rhythmic_Complexity  369 non-null    float64\n",
      " 6   Tempo                369 non-null    object \n",
      "dtypes: float64(1), object(6)\n",
      "memory usage: 20.3+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Data Pre-processing\n",
    "print(df.head())\n",
    "print(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a4fc2837",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparing the data for LSTM\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.utils import np_utils\n",
    "\n",
    "# Convert all other features to have an extra dimension for LSTM\n",
    "def transform_series(series, num_steps):\n",
    "    # Reshape series to (samples, time_steps, features)\n",
    "    X = np.zeros((len(series), num_steps, 1))\n",
    "    for i in range(len(series)):\n",
    "        X[i,:,0] = series.iloc[i][:num_steps]\n",
    "    return X\n",
    "\n",
    "\n",
    "# Separate out the test set\n",
    "# Using stratify to ensure the datasets have same prorportions of each composer as original dataset\n",
    "df_train_val, df_test = train_test_split(df, test_size=0.2, random_state=42, stratify=df['Composer'])\n",
    "\n",
    "# Second, we separate the remaining data into the train and validation sets\n",
    "df_train, df_val = train_test_split(df_train_val, test_size=0.25, random_state=42, stratify=df_train_val['Composer'])\n",
    "\n",
    "# The train/val/test split is now 60%/20%/20%\n",
    "\n",
    "# Encode the labels\n",
    "encoder = LabelEncoder()\n",
    "encoder.fit(df['Composer'])  # Fit on the whole dataset\n",
    "\n",
    "# Transform the labels to one-hot encoded form for each subset\n",
    "y_train = np_utils.to_categorical(encoder.transform(df_train['Composer']))\n",
    "y_val = np_utils.to_categorical(encoder.transform(df_val['Composer']))\n",
    "y_test = np_utils.to_categorical(encoder.transform(df_test['Composer']))\n",
    "\n",
    "# Apply transform_series on each feature for each subset\n",
    "def prepare_data(df, num_steps):\n",
    "    pitch = transform_series(df['Pitch'], num_steps)\n",
    "    note_density = transform_series(df['Note_Density'], num_steps)\n",
    "    volume = transform_series(df['Volume'], num_steps)\n",
    "    rhythmic_complexity = np.array([df['Rhythmic_Complexity'].values]*num_steps).T[:,:,np.newaxis]\n",
    "    tempo = transform_series(df['Tempo'], num_steps)\n",
    "\n",
    "    X = np.concatenate([pitch, note_density, volume, rhythmic_complexity, tempo], axis=-1)\n",
    "    return X\n",
    "\n",
    "num_steps = 27\n",
    "X_train = prepare_data(df_train, num_steps)\n",
    "X_val = prepare_data(df_val, num_steps)\n",
    "X_test = prepare_data(df_test, num_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "afd3d3da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9\n",
      "(74, 27, 5)\n"
     ]
    }
   ],
   "source": [
    "print(y_train.shape[1])\n",
    "print(X_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "12185b48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 50)                11200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 9)                 459       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 11,659\n",
      "Trainable params: 11,659\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Model Building\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dropout, Dense\n",
    "\n",
    "num_classes = y_train.shape[1]  # assuming y_train is one-hot encoded\n",
    "num_features = 5\n",
    "\n",
    "model = Sequential([\n",
    "    LSTM(50, activation='relu', input_shape=(num_steps, num_features)),\n",
    "    Dense(num_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Print a summary of the model\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "34c73d86",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - 1s 41ms/step - loss: 93.2616 - accuracy: 0.1357 - val_loss: 79.8553 - val_accuracy: 0.1216\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 0s 11ms/step - loss: 84.1676 - accuracy: 0.1448 - val_loss: 68.6948 - val_accuracy: 0.1622\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 79.8882 - accuracy: 0.0905 - val_loss: 77.2806 - val_accuracy: 0.1216\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 89.7097 - accuracy: 0.1176 - val_loss: 80.4404 - val_accuracy: 0.1216\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 90.1596 - accuracy: 0.0905 - val_loss: 97.3767 - val_accuracy: 0.1216\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 91.3193 - accuracy: 0.1222 - val_loss: 103.8171 - val_accuracy: 0.1622\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 74.3902 - accuracy: 0.1312 - val_loss: 75.1611 - val_accuracy: 0.1486\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 0s 10ms/step - loss: 77.2164 - accuracy: 0.1222 - val_loss: 77.4817 - val_accuracy: 0.1486\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 74.7617 - accuracy: 0.1131 - val_loss: 68.0184 - val_accuracy: 0.1486\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 0s 9ms/step - loss: 66.0270 - accuracy: 0.1312 - val_loss: 64.5018 - val_accuracy: 0.1351\n"
     ]
    }
   ],
   "source": [
    "# Model Training\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(X_train, y_train, epochs=10, validation_data=(X_val, y_val))  # adjust as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "f78c6da3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3/3 [==============================] - 0s 4ms/step - loss: 59.6226 - accuracy: 0.1216\n",
      "Test Loss: 59.62261199951172\n",
      "Test Accuracy: 0.12162162363529205\n",
      "3/3 [==============================] - 0s 5ms/step\n"
     ]
    }
   ],
   "source": [
    "# Model Evaluation\n",
    "# Evaluate the model\n",
    "test_loss, test_accuracy = model.evaluate(X_test, y_test)\n",
    "print('Test Loss: {}'.format(test_loss))\n",
    "print('Test Accuracy: {}'.format(test_accuracy))\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_test_pred = model.predict(X_test)\n",
    "\n",
    "# If you're doing classification and you used a softmax or sigmoid activation function in your output layer, \n",
    "# the outputs will be probabilities and you'll need to convert them to class labels\n",
    "y_test_pred_class = np.argmax(y_test_pred, axis=1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
